{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üßπ Data Cleaning ‚Äî Amazon Sales Dataset\n",
                "\n",
                "This notebook identifies data quality issues, exports bad rows to a separate CSV, fixes the main dataset, and re-runs validation.\n",
                "\n",
                "**Issues Found:**\n",
                "| Issue | Count | Fix |\n",
                "|-------|------:|-----|\n",
                "| Empty `currency` | 7,795 | Fill with `\"INR\"` |\n",
                "| Empty `Amount` | 7,795 | Fill with `0.0` (cancelled orders) |\n",
                "| Empty `ship-country` | 33 | Fill with `\"IN\"` |\n",
                "| Duplicate `Order ID` | 15,443 | Multi-item orders ‚Üí remove unique constraint |\n",
                "| Unknown `Status` values | 295 | Add missing statuses to valid list |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import importlib\n",
                "\n",
                "CSV_PATH = \"data/amazon_sales.csv\"\n",
                "BAD_ROWS_PATH = \"data/bad_rows.csv\"\n",
                "\n",
                "df = pd.read_csv(CSV_PATH, low_memory=False)\n",
                "print(f\"Rows: {len(df):,}  |  Columns: {len(df.columns)}\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Inspect Data Quality Issues"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Null counts for key columns\n",
                "key_cols = [\"Order ID\", \"Date\", \"Status\", \"Fulfilment\", \"currency\", \"Qty\", \"Amount\", \"ship-country\"]\n",
                "null_counts = df[key_cols].isnull().sum()\n",
                "print(\"=== NULL COUNTS ===\")\n",
                "print(null_counts[null_counts > 0])\n",
                "print()\n",
                "\n",
                "# Unique values for categorical columns\n",
                "print(\"=== All Status Values ===\")\n",
                "print(df[\"Status\"].value_counts(dropna=False))\n",
                "print()\n",
                "print(\"=== Currency Values ===\")\n",
                "print(df[\"currency\"].value_counts(dropna=False))\n",
                "print()\n",
                "print(\"=== Ship-Country Values ===\")\n",
                "print(df[\"ship-country\"].value_counts(dropna=False))\n",
                "print()\n",
                "print(\"=== Duplicate Order IDs ===\")\n",
                "dup_count = df[\"Order ID\"].duplicated().sum()\n",
                "print(f\"{dup_count:,} duplicate Order ID rows (multi-item orders)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Identify & Export Bad Rows"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define conditions for bad rows\n",
                "mask_null_currency = df[\"currency\"].isnull()\n",
                "mask_null_amount = df[\"Amount\"].isnull()\n",
                "mask_null_country = df[\"ship-country\"].isnull()\n",
                "mask_null_order_id = df[\"Order ID\"].isnull()\n",
                "mask_neg_qty = df[\"Qty\"] < 0\n",
                "\n",
                "# Combine all bad-row conditions\n",
                "bad_mask = (\n",
                "    mask_null_currency\n",
                "    | mask_null_amount\n",
                "    | mask_null_country\n",
                "    | mask_null_order_id\n",
                "    | mask_neg_qty\n",
                ")\n",
                "\n",
                "bad_rows = df[bad_mask].copy()\n",
                "bad_rows[\"issue\"] = \"\"\n",
                "bad_rows.loc[mask_null_currency, \"issue\"] += \"null_currency; \"\n",
                "bad_rows.loc[mask_null_amount, \"issue\"] += \"null_amount; \"\n",
                "bad_rows.loc[mask_null_country, \"issue\"] += \"null_ship_country; \"\n",
                "bad_rows.loc[mask_null_order_id, \"issue\"] += \"null_order_id; \"\n",
                "bad_rows.loc[mask_neg_qty, \"issue\"] += \"negative_qty; \"\n",
                "\n",
                "print(f\"Total bad rows: {len(bad_rows):,}\")\n",
                "print()\n",
                "print(\"Issue breakdown:\")\n",
                "print(f\"  Null currency:     {mask_null_currency.sum():,}\")\n",
                "print(f\"  Null Amount:       {mask_null_amount.sum():,}\")\n",
                "print(f\"  Null ship-country: {mask_null_country.sum():,}\")\n",
                "print(f\"  Null Order ID:     {mask_null_order_id.sum():,}\")\n",
                "print(f\"  Negative Qty:      {mask_neg_qty.sum():,}\")\n",
                "print()\n",
                "bad_rows.head(10)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Export bad rows to a separate CSV\n",
                "bad_rows.to_csv(BAD_ROWS_PATH, index=False)\n",
                "print(f\"‚úÖ Exported {len(bad_rows):,} bad rows to: {BAD_ROWS_PATH}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Fix Data Issues"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Fix 1: Fill empty currency with \"INR\"\n",
                "before = df[\"currency\"].isnull().sum()\n",
                "df[\"currency\"] = df[\"currency\"].fillna(\"INR\")\n",
                "print(f\"‚úÖ currency: {before} nulls ‚Üí {df['currency'].isnull().sum()} nulls\")\n",
                "\n",
                "# Fix 2: Fill empty Amount with 0.0 (cancelled orders)\n",
                "before = df[\"Amount\"].isnull().sum()\n",
                "df[\"Amount\"] = df[\"Amount\"].fillna(0.0)\n",
                "print(f\"‚úÖ Amount: {before} nulls ‚Üí {df['Amount'].isnull().sum()} nulls\")\n",
                "\n",
                "# Fix 3: Fill empty ship-country with \"IN\"\n",
                "before = df[\"ship-country\"].isnull().sum()\n",
                "df[\"ship-country\"] = df[\"ship-country\"].fillna(\"IN\")\n",
                "print(f\"‚úÖ ship-country: {before} nulls ‚Üí {df['ship-country'].isnull().sum()} nulls\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Verify Fixes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Final null check\n",
                "null_counts_after = df[key_cols].isnull().sum()\n",
                "remaining = null_counts_after[null_counts_after > 0]\n",
                "\n",
                "print(\"=== REMAINING NULLS ===\")\n",
                "if remaining.empty:\n",
                "    print(\"‚úÖ No nulls remaining in key columns!\")\n",
                "else:\n",
                "    print(remaining)\n",
                "\n",
                "print()\n",
                "print(\"=== VALUE CHECKS ===\")\n",
                "print(f\"Unique currencies:     {df['currency'].unique()}\")\n",
                "print(f\"Unique ship-countries: {df['ship-country'].unique()}\")\n",
                "print(f\"Negative Qty count:    {(df['Qty'] < 0).sum()}\")\n",
                "print(f\"Negative Amount count: {(df['Amount'] < 0).sum()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Save Cleaned Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save cleaned data (overwrite original)\n",
                "df.to_csv(CSV_PATH, index=False)\n",
                "print(f\"‚úÖ Cleaned data saved to: {CSV_PATH}\")\n",
                "print(f\"   Rows: {len(df):,}  |  Columns: {len(df.columns)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Re-run Validation Pipeline\n",
                "\n",
                "> ‚ö†Ô∏è **Important:** If you edited the source modules, restart the kernel before running this cell so the latest code is loaded."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Force-reload modules to pick up any code changes\n",
                "import src.ge_validation as ge_mod\n",
                "import src.pydantic_validation as py_mod\n",
                "importlib.reload(ge_mod)\n",
                "importlib.reload(py_mod)\n",
                "\n",
                "from src.ge_validation import run_ge_validation\n",
                "from src.pydantic_validation import run_pydantic_validation\n",
                "\n",
                "df_clean = pd.read_csv(CSV_PATH, low_memory=False)\n",
                "\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"   RE-RUNNING VALIDATION ON CLEANED DATA\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "ge_summary = run_ge_validation(df_clean)\n",
                "pydantic_summary = run_pydantic_validation(df_clean)\n",
                "\n",
                "all_ok = ge_summary[\"overall_success\"] and pydantic_summary[\"overall_success\"]\n",
                "\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"   FINAL RESULT\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"   GE Validation      : {'‚úÖ' if ge_summary['overall_success'] else '‚ùå'}\")\n",
                "print(f\"   Pydantic Validation : {'‚úÖ' if pydantic_summary['overall_success'] else '‚ùå'}\")\n",
                "print(f\"   Overall             : {'‚úÖ ALL PASSED' if all_ok else '‚ùå ISSUES FOUND'}\")\n",
                "print(\"=\" * 60)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üßπ Data Cleaning ‚Äî Amazon Sales Dataset\n",
                "\n",
                "This notebook identifies data quality issues, exports bad rows to a separate CSV, cleans the main dataset, and re-runs the validation pipeline.\n",
                "\n",
                "> ‚ö†Ô∏è **Run this notebook BEFORE running `python dq_pipeline.py`** to ensure the data is clean."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import importlib\n",
                "\n",
                "CSV_PATH = \"data/amazon_sales.csv\"\n",
                "BAD_ROWS_PATH = \"data/bad_rows.csv\"\n",
                "\n",
                "df = pd.read_csv(CSV_PATH, low_memory=False)\n",
                "print(f\"Rows: {len(df):,}  |  Columns: {len(df.columns)}\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Inspect Data Quality Issues"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "key_cols = [\"Order ID\", \"Date\", \"Status\", \"Fulfilment\", \"currency\", \"Qty\", \"Amount\", \"ship-country\"]\n",
                "\n",
                "print(\"=== NULL COUNTS ===\")\n",
                "null_counts = df[key_cols].isnull().sum()\n",
                "print(null_counts[null_counts > 0])\n",
                "print()\n",
                "print(\"=== All Status Values ===\")\n",
                "print(df[\"Status\"].value_counts(dropna=False))\n",
                "print()\n",
                "print(\"=== Currency Values ===\")\n",
                "print(df[\"currency\"].value_counts(dropna=False))\n",
                "print()\n",
                "print(\"=== Ship-Country Values ===\")\n",
                "print(df[\"ship-country\"].value_counts(dropna=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Identify & Export Bad Rows"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Identify all rows with any issue\n",
                "mask_null_currency = df[\"currency\"].isnull()\n",
                "mask_null_amount = df[\"Amount\"].isnull()\n",
                "mask_null_country = df[\"ship-country\"].isnull()\n",
                "mask_null_order_id = df[\"Order ID\"].isnull()\n",
                "mask_neg_qty = df[\"Qty\"] < 0\n",
                "\n",
                "bad_mask = mask_null_currency | mask_null_amount | mask_null_country | mask_null_order_id | mask_neg_qty\n",
                "\n",
                "bad_rows = df[bad_mask].copy()\n",
                "bad_rows[\"issue\"] = \"\"\n",
                "bad_rows.loc[mask_null_currency, \"issue\"] += \"null_currency; \"\n",
                "bad_rows.loc[mask_null_amount, \"issue\"] += \"null_amount; \"\n",
                "bad_rows.loc[mask_null_country, \"issue\"] += \"null_ship_country; \"\n",
                "bad_rows.loc[mask_null_order_id, \"issue\"] += \"null_order_id; \"\n",
                "bad_rows.loc[mask_neg_qty, \"issue\"] += \"negative_qty; \"\n",
                "\n",
                "print(f\"Total bad rows found: {len(bad_rows):,}\")\n",
                "print(f\"  Null currency:     {mask_null_currency.sum():,}\")\n",
                "print(f\"  Null Amount:       {mask_null_amount.sum():,}\")\n",
                "print(f\"  Null ship-country: {mask_null_country.sum():,}\")\n",
                "print(f\"  Null Order ID:     {mask_null_order_id.sum():,}\")\n",
                "print(f\"  Negative Qty:      {mask_neg_qty.sum():,}\")\n",
                "print()\n",
                "bad_rows.head(10)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Export bad rows to a separate CSV for reference\n",
                "bad_rows.to_csv(BAD_ROWS_PATH, index=False)\n",
                "print(f\"‚úÖ Exported {len(bad_rows):,} bad rows ‚Üí {BAD_ROWS_PATH}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Fix Data Issues\n",
                "\n",
                "| Fix | Column | Action | Reason |\n",
                "|-----|--------|--------|--------|\n",
                "| 1 | `currency` | Fill NaN ‚Üí `\"INR\"` | All valid rows use INR |\n",
                "| 2 | `Amount` | Fill NaN ‚Üí `0.0` | Cancelled orders have no amount |\n",
                "| 3 | `ship-country` | Fill NaN ‚Üí `\"IN\"` | All valid rows use IN |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Apply fixes\n",
                "df[\"currency\"]     = df[\"currency\"].fillna(\"INR\")\n",
                "df[\"Amount\"]       = df[\"Amount\"].fillna(0.0)\n",
                "df[\"ship-country\"] = df[\"ship-country\"].fillna(\"IN\")\n",
                "\n",
                "print(\"‚úÖ All fixes applied!\")\n",
                "print()\n",
                "print(\"Remaining nulls in key columns:\")\n",
                "remaining = df[key_cols].isnull().sum()\n",
                "remaining = remaining[remaining > 0]\n",
                "print(\"  None! ‚úÖ\" if remaining.empty else remaining)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Save Cleaned Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.to_csv(CSV_PATH, index=False)\n",
                "print(f\"‚úÖ Cleaned data saved ‚Üí {CSV_PATH}\")\n",
                "print(f\"   {len(df):,} rows  |  {len(df.columns)} columns\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Re-run Validation Pipeline\n",
                "\n",
                "Reload the modules to pick up any code changes, then validate the cleaned data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Force reload modules (picks up code changes without kernel restart)\n",
                "import src.ge_validation as _ge\n",
                "import src.pydantic_validation as _py\n",
                "importlib.reload(_ge)\n",
                "importlib.reload(_py)\n",
                "\n",
                "# Re-read the cleaned CSV\n",
                "df_clean = pd.read_csv(CSV_PATH, low_memory=False)\n",
                "\n",
                "print(\"=\" * 60)\n",
                "print(\"   RE-RUNNING VALIDATION ON CLEANED DATA\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "ge_summary = _ge.run_ge_validation(df_clean)\n",
                "pydantic_summary = _py.run_pydantic_validation(df_clean)\n",
                "\n",
                "all_ok = ge_summary[\"overall_success\"] and pydantic_summary[\"overall_success\"]\n",
                "\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"   FINAL RESULT\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"   GE Validation      : {'‚úÖ' if ge_summary['overall_success'] else '‚ùå'}\")\n",
                "print(f\"   Pydantic Validation : {'‚úÖ' if pydantic_summary['overall_success'] else '‚ùå'}\")\n",
                "print(f\"   Overall             : {'‚úÖ ALL PASSED' if all_ok else '‚ùå ISSUES FOUND'}\")\n",
                "print(\"=\" * 60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Done!\n",
                "\n",
                "Now you can run `python dq_pipeline.py` from the terminal and it should pass ‚úÖ"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}